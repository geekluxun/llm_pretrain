{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 简单的训练一个分词器\n",
    "import sentencepiece as spm\n",
    "\n",
    "input = '/Users/luxun/workspace/ai/mine/llm_pretrain/.temp/the-verdict.txt'\n",
    "spm.SentencePieceTrainer.train(input=input, model_prefix='m', vocab_size=1000, user_defined_symbols=['foo', 'bar'],\n",
    "                               model_type='bpe')"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#使用训练好的分词器\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "id = sp.encode('hello world')\n",
    "print(id)\n",
    "s = sp.decode(id)\n",
    "s"
   ],
   "id": "8eb07c263e7ff9ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 加载一个tiktoken分词器（）\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "enc.encode(\"aa d\")"
   ],
   "id": "dd41f4291a9d51a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 加载一个transformers分词器\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# 加载预训练分词器，这里以 BERT 的分词器为例\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"/Users/luxun/workspace/ai/mine/llm_pretrain/notebook/tokenizer.json\")\n",
    "\n",
    "# 输入文本\n",
    "text = \"这是一个中文句子.\"\n",
    "\n",
    "# 1. 使用 tokenizer.tokenize\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokenized result:\", tokens)\n",
    "# 输出: ['this', 'is', 'a', 'test', 'sentence', '.']\n",
    "\n",
    "# 2. 使用 tokenizer.encode\n",
    "token_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "print(\"Encoded result (token IDs):\", token_ids)\n",
    "# 输出: [101, 2023, 2003, 1037, 3231, 6251, 1012, 102] （以 BERT 为例）\n",
    "\n",
    "# 3. 解码 token IDs\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"Decoded text:\", decoded_text)\n",
    "# 输出: \"this is a test sentence.\"\n"
   ],
   "id": "24c7940928095283",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import chardet\n",
    "from charset_normalizer import from_bytes\n",
    "\n",
    "\n",
    "def detect_file_encoding(file_path):\n",
    "    \"\"\"\n",
    "    检测文件编码。\n",
    "    :param file_path: 文件路径\n",
    "    :return: 编码类型\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            raw_data = file.read()\n",
    "\n",
    "            # 方法 1: 使用 chardet 检测\n",
    "            chardet_result = chardet.detect(raw_data)\n",
    "            chardet_encoding = chardet_result['encoding']\n",
    "\n",
    "            # 方法 2: 使用 charset-normalizer 检测\n",
    "            charset_result = from_bytes(raw_data).best()\n",
    "            charset_encoding = charset_result.encoding if charset_result else None\n",
    "\n",
    "            # 返回检测结果\n",
    "            return {\n",
    "                \"chardet_encoding\": chardet_encoding,\n",
    "                \"charset_normalizer_encoding\": charset_encoding\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "# 示例使用\n",
    "file_path = \"/Users/luxun/workspace/ai/mine/llm_pretrain/notebook/tokenizer.json\"\n",
    "result = detect_file_encoding(file_path)\n",
    "print(\"Encoding detection result:\", result)"
   ],
   "id": "7b2a7d9131fb6f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T07:08:12.645858Z",
     "start_time": "2024-11-30T07:08:12.599364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用tokenizers BPE训练一个分词器\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# 初始化 BPE 分词器\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "# 设置正则化器（可选）\n",
    "tokenizer.normalizer = NFKC()\n",
    "\n",
    "# 设置预分词器\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# 定义 BPE 训练器\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=30000,  # 词汇表大小\n",
    "    min_frequency=2,  # 最低频率\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],  # 特殊标记\n",
    ")\n",
    "\n",
    "# 加载训练数据\n",
    "files = [\"/Users/luxun/workspace/ai/mine/llm_pretrain/.temp/the-verdict.txt\"]\n",
    "\n",
    "# 开始训练\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "# 设置 post-processing（例如模板处理）\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 保存分词器\n",
    "tokenizer.save(\"tokenizer-bpe.json\")\n",
    "\n",
    "print(\"BPE 分词器训练完成并保存为 tokenizer-bpe.json\")\n"
   ],
   "id": "23e2c5abfe4bbf91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "BPE 分词器训练完成并保存为 tokenizer-bpe.json\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T07:08:58.800098Z",
     "start_time": "2024-11-30T07:08:58.795679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer-bpe.json\")\n",
    "\n",
    "# 对文本进行分词\n",
    "output = tokenizer.encode(\"hello\")\n",
    "print(\"Tokens:\", output.tokens)\n",
    "print(\"IDs:\", output.ids)\n"
   ],
   "id": "a50fb7ede17f90b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'he', 'll', 'o', '[SEP]']\n",
      "IDs: [2, 65, 95, 53, 3]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "333fcc59bad30036"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
